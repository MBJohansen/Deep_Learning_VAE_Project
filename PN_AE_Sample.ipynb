{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(sns.dark_palette(\"purple\"))\n",
    "\n",
    "import torch\n",
    "iscuda = torch.cuda.is_available()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.transforms import ToTensor\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = io.open(\"Data\\PoE\\4ggNLmuc.txt\",\"r\", encoding='utf8').read()\n",
    "p2 = io.open(\"Data\\PoE\\pathofexile_3.8_patch_notes.txt\",\"r\",encoding='utf8').read()\n",
    "p3 = io.open(\"Data\\PoE\\poe_3.7_patch_notes.txt\",\"r\", encoding='utf8').read()\n",
    "\n",
    "dataset = p1.split(\"\\n\") + p2.split(\"\\n\") + p3.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for d in dataset:\n",
    "    test = word_tokenize(d)\n",
    "\n",
    "    full_sent = ' '.join(test)\n",
    "    token_sent = sent_tokenize(full_sent)\n",
    "\n",
    "    sentences = sentences + token_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for d in sentences:\n",
    "    words = d.split(' ')\n",
    "    sequences.extend(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    \n",
    "# Flatten the dataset\n",
    "all_words = flatten(sequences)\n",
    "\n",
    "\n",
    "testSet = set()\n",
    "for x in all_words:\n",
    "    testSet.add(x.lower())\n",
    "\n",
    "testSet.add(' ')\n",
    "testSet.add('\\\"')\n",
    "testSet.add('UNK')\n",
    "    \n",
    "len(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "every_word = set()\n",
    "\n",
    "for word in sequences:\n",
    "    every_word.add(word.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3872"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(every_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 48075 sentences and 3873 unique tokens in our dataset (including UNK).\n",
      "\n",
      "The index of 'the' is 3418\n",
      "The word corresponding to index 1 is 'skewering'\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# This was based on the course work\n",
    "\n",
    "def sequences_to_dicts(sequences):\n",
    "    \"\"\"\n",
    "    Creates word_to_idx and idx_to_word dictionaries for a list of sequences.\n",
    "    \"\"\"\n",
    "     \n",
    "    \n",
    "    # A bit of Python-magic to flatten a nested list\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    \n",
    "    # Flatten the dataset\n",
    "    all_words = flatten(sequences)\n",
    "    \n",
    "    # Count number of word occurences\n",
    "    word_count = defaultdict(int)\n",
    "    for word in flatten(sequences):\n",
    "        word_count[word] += 1\n",
    "\n",
    "    # Sort by frequency\n",
    "    word_count = sorted(list(word_count.items()), key=lambda l: -l[1])\n",
    "\n",
    "    # Create a list of all unique words\n",
    "    unique_words = [item[0] for item in word_count]\n",
    "    \n",
    "    # Add UNK token to list of words\n",
    "    unique_words.append('UNK')\n",
    "\n",
    "    unique_words.append(' ')\n",
    "    unique_words.append('\\\"')\n",
    "\n",
    "    every_word = set()\n",
    "\n",
    "    for word in sequences:\n",
    "        every_word.add(word.lower())\n",
    "    \n",
    "    every_word.add('UNK')\n",
    "\n",
    "    unique_words = every_word\n",
    "    \n",
    "    # Count number of sequences and number of unique words\n",
    "    num_sentences, vocab_size = len(sequences), len(unique_words)\n",
    "\n",
    "    # Create dictionaries so that we can go from word to index and back\n",
    "    # If a word is not in our vocabulary, we assign it to token 'UNK'\n",
    "    word_to_idx = defaultdict(lambda: num_sentences)\n",
    "    idx_to_word = defaultdict(lambda: 'UNK')\n",
    "\n",
    "    # Fill dictionaries\n",
    "    for idx, word in enumerate(unique_words):\n",
    "        word_to_idx[word] = idx \n",
    "        idx_to_word[idx] = word\n",
    "\n",
    "    return word_to_idx, idx_to_word, num_sentences, vocab_size\n",
    "\n",
    "\n",
    "word_to_idx, idx_to_word, num_sequences, vocab_size = sequences_to_dicts(sequences)\n",
    "\n",
    "print(f'We have {num_sequences} sentences and {len(word_to_idx)} unique tokens in our dataset (including UNK).\\n')\n",
    "print('The index of \\'the\\' is', word_to_idx['the'])\n",
    "print(f'The word corresponding to index 1 is \\'{idx_to_word[1]}\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array([word_to_idx[c] for c in all_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import random\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve inputs and targets at the given index\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "def create_datasets(sequences, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1):\n",
    "    # Define partition sizes\n",
    "    num_train = int(len(sequences)*p_train)\n",
    "    num_val = int(len(sequences)*p_val)\n",
    "    num_test = int(len(sequences)*p_test)\n",
    "\n",
    "    random_sequence = sequences\n",
    "    random.shuffle(random_sequence)\n",
    "    \n",
    "    # Split sequences into partitions\n",
    "    sequences_train = random_sequence[:num_train]\n",
    "    sequences_val = random_sequence[num_train:num_train+num_val]\n",
    "    sequences_test = random_sequence[-num_test:]\n",
    "\n",
    "    def get_inputs_targets_from_sequences(sequences):\n",
    "        # Define empty lists\n",
    "        inputs, targets = [], []\n",
    "        \n",
    "        # Append inputs and targets s.t. both lists contain L-1 words of a sentence of length L\n",
    "        # but targets are shifted right by one so that we can predict the next word\n",
    "        \n",
    "        for sequence in sequences:\n",
    "            sequence = sequence.lower()\n",
    "            sequence_list = sequence.split(' ')\n",
    "            if len(sequence_list) < 2:\n",
    "                continue\n",
    "            \n",
    "            inputs.append(sequence_list[:-1])\n",
    "            targets.append(sequence_list[1:])\n",
    "            \n",
    "        return inputs, targets\n",
    "\n",
    "    # Get inputs and targets for each partition\n",
    "    inputs_train, targets_train = get_inputs_targets_from_sequences(sequences_train)\n",
    "    inputs_val, targets_val = get_inputs_targets_from_sequences(sequences_val)\n",
    "    inputs_test, targets_test = get_inputs_targets_from_sequences(sequences_test)\n",
    "\n",
    "    # Create datasets\n",
    "    training_set = dataset_class(inputs_train, targets_train)\n",
    "    validation_set = dataset_class(inputs_val, targets_val)\n",
    "    test_set = dataset_class(inputs_test, targets_test)\n",
    "\n",
    "    return training_set, validation_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2217 samples in the training set.\n",
      "We have 283 samples in the validation set.\n",
      "We have 282 samples in the test set.\n"
     ]
    }
   ],
   "source": [
    "training_set, validation_set, test_set = create_datasets(sentences, Dataset)\n",
    "\n",
    "print(f'We have {len(training_set)} samples in the training set.')\n",
    "print(f'We have {len(validation_set)} samples in the validation set.')\n",
    "print(f'We have {len(test_set)} samples in the test set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lstm): LSTM(3873, 968)\n",
      "  (l_out): Linear(in_features=968, out_features=3873, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "hidden_dim = int(vocab_size/4)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=vocab_size, hidden_size=hidden_dim) \n",
    "        \n",
    "        # Output layer\n",
    "        self.l_out = nn.Linear(in_features=hidden_dim,\n",
    "                            out_features=vocab_size,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, (h, c) = self.lstm(x)\n",
    "        \n",
    "        # Flatten output for feed-forward layer\n",
    "        x = x.view(-1, self.lstm.hidden_size)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.l_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our one-hot encoding of 'the' has shape (3873,).\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Our one-hot encoding of 'a b' has shape (12, 3873, 1).\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "     `idx`: the index of the given word\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "    \n",
    "    Returns a 1-D numpy array of length `vocab_size`.\n",
    "    \"\"\"\n",
    "    # Initialize the encoded array\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    \n",
    "    # Set the appropriate element to one\n",
    "    one_hot[idx] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
    "    \n",
    "    Args:\n",
    "     `sentence`: a list of words to encode\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "     \n",
    "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode each word in the sentence\n",
    "    encoding = np.array([one_hot_encode(word_to_idx[word.lower()], vocab_size) for word in sequence])\n",
    "\n",
    "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "\n",
    "test_word = one_hot_encode(word_to_idx['the'], vocab_size)\n",
    "print(f'Our one-hot encoding of \\'the\\' has shape {test_word.shape}.')\n",
    "print(test_word)\n",
    "\n",
    "test_sentence = one_hot_encode_sequence(['now', 'has', '20', 'chance', 'for', 'the', 'non-primary', 'projectiles', 'to', 'impale', 'enemies', 'on'], vocab_size)\n",
    "print(f'Our one-hot encoding of \\'a b\\' has shape {test_sentence.shape}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'radius', 'of', 'the', 'explosions', 'has', 'been', 'reduced', 'from', '18', 'to', '16', 'and', 'the', 'explosions', 'now', 'have', 'a', '20', '%', 'variance', 'in', 'area', 'of', 'effect']\n",
      "Our one-hot encoding of  has shape (25, 3873, 1).\n"
     ]
    }
   ],
   "source": [
    "testSeq = training_set.inputs[12]\n",
    "print(testSeq)\n",
    "test_sentence = one_hot_encode_sequence(testSeq, vocab_size)\n",
    "print(f'Our one-hot encoding of  has shape {test_sentence.shape}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 5.1251737878741475, validation loss: 8.261446022734626\n",
      "Epoch 5, training loss: 1.2933734369795664, validation loss: 4.3567371635538175\n",
      "Epoch 10, training loss: 0.6894760324619605, validation loss: 4.613604761624722\n",
      "Epoch 15, training loss: 0.4598108694876863, validation loss: 4.874066871482313\n",
      "Epoch 20, training loss: 0.3744234936499073, validation loss: 4.861380245229381\n",
      "Epoch 25, training loss: 0.3501234817346419, validation loss: 5.010097397669044\n",
      "Epoch 30, training loss: 0.31467748562630304, validation loss: 5.1470649909849655\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 35\n",
    "\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.005)\n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "    net.eval()\n",
    "        \n",
    "    # For each sentence in validation set\n",
    "    for inputs, targets in validation_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_idx = [word_to_idx[word] for word in targets]\n",
    "        \n",
    "        # Convert input to tensor\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot).cuda()\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Convert target to tensor\n",
    "        targets_idx = torch.LongTensor(targets_idx).cuda()\n",
    "        \n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net.forward(inputs_one_hot)\n",
    "\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets_idx)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in training_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_idx = [word_to_idx[word] for word in targets]\n",
    "        \n",
    "        # Convert input to tensor\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot).cuda()\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "        \n",
    "        # Convert target to tensor\n",
    "        targets_idx = torch.LongTensor(targets_idx).cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net.forward(inputs_one_hot) \n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets_idx)\n",
    "        \n",
    "        # Backward pass\n",
    "        # zero grad, backward, step...\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() \n",
    "        \n",
    "        _ = torch.nn.utils.clip_grad_norm_(\n",
    "                net.parameters(), 5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_training_loss += loss.detach().cpu().numpy()\n",
    "        \n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Print loss every 5 epochs\n",
    "    if i % 5 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get first sentence in test set\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_idx = [word_to_idx[word] for word in targets]\n",
    "\n",
    "# Convert input to tensor\n",
    "inputs_one_hot = torch.Tensor(inputs_one_hot).cuda()\n",
    "inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "\n",
    "# Convert target to tensor\n",
    "targets_idx = torch.LongTensor(targets_idx).cuda()\n",
    "\n",
    "# Forward pass\n",
    "outputs = net.forward(inputs_one_hot).data.cpu().numpy()\n",
    "\n",
    "\n",
    "# Plot training and validation loss\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input sequence:\n",
      "shares its cooldown with other guard skills ( steelskin and immortal call )\n",
      "\n",
      "Target sequence:\n",
      "its cooldown with other guard skills ( steelskin and immortal call ) .\n",
      "\n",
      "Predicted sequence:\n",
      "its cooldown with other guard skills ( steelskin and molten call ) .\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = training_set[50]\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_idx = [word_to_idx[word] for word in targets]\n",
    "\n",
    "# Convert input to tensor\n",
    "inputs_one_hot = torch.Tensor(inputs_one_hot).cuda()\n",
    "inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "\n",
    "# Convert target to tensor\n",
    "targets_idx = torch.LongTensor(targets_idx).cuda()\n",
    "\n",
    "# Forward pass\n",
    "outputs = net.forward(inputs_one_hot).data.cpu().numpy()\n",
    "\n",
    "inputstr = ' '.join(inputs)\n",
    "\n",
    "targetstr = ' '.join(targets)\n",
    "\n",
    "predictedSeq = [idx_to_word[np.argmax(output)] for output in outputs]\n",
    "teststr = ' '.join(predictedSeq)\n",
    "\n",
    "print('\\nInput sequence:')\n",
    "print(inputstr)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(targetstr)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print(teststr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elemental overload now causes ailments to never count as being from critical strikes ( previously caused ailments from critical strikes to have no damage multiplier ) . poison at gem level\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "full_story = 'elemental'\n",
    "phrase = [full_story]\n",
    "\n",
    "while count < 30:\n",
    "    net.eval()\n",
    "    inputs = phrase\n",
    "    \n",
    "    inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "\n",
    "    # Convert input to tensor\n",
    "    inputs_one_hot = torch.Tensor(inputs_one_hot).cuda()\n",
    "    inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "    outputs = net.forward(inputs_one_hot).data.cpu().numpy()\n",
    "        \n",
    "    next_word = outputs[-1]\n",
    "    \n",
    "    floor = min(next_word)\n",
    "    next_word = [x - floor for x in next_word]\n",
    "    next_word /= sum(next_word)\n",
    "    count+=1\n",
    "    \n",
    "    word_index = np.argmax(next_word)\n",
    "    \n",
    "    \n",
    "    predictedSeq = idx_to_word[word_index]\n",
    "    \n",
    "    \n",
    "    teststr = ' '.join(predictedSeq)\n",
    "    full_story += ' ' + predictedSeq\n",
    "    phrase = full_story.split(' ')\n",
    "\n",
    "print(full_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
